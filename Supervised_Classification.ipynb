{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2df45e03-458b-473a-97bc-c2a378ba8d09",
   "metadata": {},
   "source": [
    "Supervised Classification: Decision \n",
    "Trees, SVM, and Naive Baye|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9bf96a-4c5d-4d76-9ce8-19b4987dfdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Question 1 : What is Information Gain, and how is it used in Decision Trees?\n",
    "Answer:      Information Gain measures how much uncertainty about the target variable is reduced after splitting the data based on a particular feature. \n",
    "             In Decision Trees, it helps determine which attribute to split on at each node. The feature with the highest Information Gain — \n",
    "             meaning it best separates the classes — is chosen for the split, leading to a purer classification.\n",
    "\n",
    "Question 2: What is the difference between Gini Impurity and Entropy?\n",
    "Answer:     Both Gini Impurity and Entropy measure how mixed the classes are within a node. Entropy uses logarithms and measures information content, \n",
    "            while Gini measures the probability of incorrect classification. Gini is computationally faster and often preferred in CART, \n",
    "            while Entropy is used in ID3/C4.5 and gives slightly more balanced splits.\n",
    "\n",
    "Question 3:What is Pre-Pruning in Decision Trees?\n",
    "Answer:    Pre-pruning in Decision Trees is a technique used to stop the tree from growing too large during its construction. \n",
    "           It sets conditions such as maximum depth, minimum number of samples required to split a node, or minimum Information Gain. \n",
    "           This prevents overfitting by halting further splits that do not significantly improve model accuracy, resulting in a simpler \n",
    "           and more generalizable tree.           \n",
    "'''           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bc469cd-478c-4084-bd3d-92285fa06e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "sepal length (cm): 0.0000\n",
      "sepal width (cm): 0.0191\n",
      "petal length (cm): 0.8933\n",
      "petal width (cm): 0.0876\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Question 4:Write a Python program to train a Decision Tree Classifier using Gini\n",
    "Impurity as the criterion and print the feature importances (practical).\n",
    "Answer:\n",
    "'''\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "for name, importance in zip(iris.feature_names, model.feature_importances_):\n",
    "    print(f\"{name}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea563edf-1780-440f-ba0b-877eba14a9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Question 5: What is a Support Vector Machine (SVM)?\n",
    "Answer:     A Support Vector Machine (SVM) is a supervised learning algorithm used for classification and regression tasks. \n",
    "            It works by finding the optimal hyperplane that best separates data points of different classes with the maximum margin. \n",
    "            The data points closest to this boundary are called support vectors, as they directly influence the position of the hyperplane. \n",
    "            SVMs are effective in high-dimensional spaces and can handle both linear and non-linear data using kernel functions.\n",
    "\n",
    "Question 6: What is the Kernel Trick in SVM?\n",
    "Answer:     The Kernel Trick in SVM is a mathematical technique that allows the algorithm to handle non-linear data by implicitly mapping \n",
    "            it into a higher-dimensional space without explicitly performing the transformation. This helps the SVM find a linear separating \n",
    "            hyperplane in that transformed space. Common kernel functions include linear, polynomial, and RBF (Radial Basis Function) kernels, \n",
    "            which enable SVMs to model complex, non-linear decision boundaries efficiently.\n",
    "'''            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f2a4792-07cc-4d81-89a3-a7bcb7e74a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Kernel Accuracy: 0.9555555555555556\n",
      "RBF Kernel Accuracy: 0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Question 7: Write a Python program to train two SVM classifiers with Linear and RBF\n",
    "kernels on the Wine dataset, then compare their accuracies.\n",
    "Answer:\n",
    "'''\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "wine = load_wine()\n",
    "X, y = wine.data, wine.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "svm_linear = SVC(kernel='linear', random_state=42)\n",
    "svm_linear.fit(X_train, y_train)\n",
    "linear_acc = svm_linear.score(X_test, y_test)\n",
    "\n",
    "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
    "svm_rbf.fit(X_train, y_train)\n",
    "rbf_acc = svm_rbf.score(X_test, y_test)\n",
    "\n",
    "print(\"Linear Kernel Accuracy:\", linear_acc)\n",
    "print(\"RBF Kernel Accuracy:\", rbf_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76662b8-1b84-45bf-a1f1-7324cd1ad31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Question 8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
    "Answer:     The Naïve Bayes classifier is a probabilistic machine learning algorithm based on Bayes’ Theorem, which predicts\n",
    "            the class of a data point by calculating the probability of each class given the input features. It is called “Naïve” \n",
    "            because it assumes that all features are independent of each other given the class label — an assumption that is rarely \n",
    "            true in real-world data but simplifies computation. Despite this simplification, Naïve Bayes often performs remarkably well, \n",
    "            especially in text classification and spam detection.\n",
    "\n",
    "Question 9: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes\n",
    "Answer:     The three types of Naïve Bayes classifiers differ based on the type of data they handle. Gaussian Naïve Bayes is used for\n",
    "            continuous data and assumes that features follow a normal (Gaussian) distribution. Multinomial Naïve Bayes is suitable for \n",
    "            count-based features, such as word frequencies in text classification. Bernoulli Naïve Bayes is designed for binary features, \n",
    "            where each feature represents the presence or absence of a particular attribute (e.g., whether a word appears in a document).\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ff93a21-19b0-4d4d-9715-dc710e4f666e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.958041958041958\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Question 10: Breast Cancer Dataset\n",
    "Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer\n",
    "dataset and evaluate accuracy.\n",
    "Answer:\n",
    "'''\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.25, random_state=42)\n",
    "model = GaussianNB().fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy:\", model.score(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
